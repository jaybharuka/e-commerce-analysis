# ğŸ›’ E-Commerce Data Pipeline with ML

## ğŸ“„ Description
This project implements an advanced ELT (Extract, Load, Transform) data pipeline for analyzing e-commerce transaction data from a UK-based online retailer. It uses a modern, containerized tech stack to enable efficient data ingestion, transformation, warehousing, and visualization. The pipeline now includes **5 machine learning models** that provide actionable insights for customer segmentation, sales forecasting, product recommendations, churn prediction, and anomaly detection.

---

## ğŸš€ Features
- **Data Extraction**: Ingest raw e-commerce transaction data from CSV files.
- **Data Transformation**: Clean, aggregate, and enrich data using Apache Spark.
- **Data Warehousing**: Store processed data in Hive tables for querying.
- **Workflow Orchestration**: Manage pipeline tasks using Apache Airflow.
- **Machine Learning**: 5 production-ready ML models for business insights
  - ğŸ‘¥ **Customer Segmentation** (K-Means clustering)
  - ğŸ“ˆ **Sales Forecasting** (Linear Regression)
  - ğŸ¯ **Product Recommendations** (ALS collaborative filtering)
  - âš ï¸ **Churn Prediction** (Random Forest classifier)
  - ğŸš¨ **Anomaly Detection** (Statistical z-score analysis)
- **Interactive Dashboard**: Streamlit-powered analytics with ML insights
- **Scalability**: Leverage Hadoop for distributed data storage and processing.
- **Containerized Deployment**: Use Docker to run the entire stack seamlessly.

---
## ğŸ“ Project Architecture
![Airflow Pipeline Graph](/output/ELT-Pipeline-Workflow.png)
*ELT pipeline workflow with integrated ML models*

---

## ğŸ› ï¸ Tech Stack

| Technology | Purpose | Version |
|--- |--- | --- |
| Hadoop | Distributed storage (HDFS) | 3.2.1 |
| Python | Programming and scripting  | 3.9 |
| Spark | Distributed data processing | 3.2.2
| PySpark MLlib | Machine learning library | 3.2.2
| Airflow | Workflow orchestration | 2.3.3 |
| Zeppelin | Web-based notebook for exploration | 0.10.1 |
| Hive | Data warehousing solution | 2.3.2 |
| Postgres | Hive metastore backend | 15.1 |
| Streamlit | Interactive dashboard | Latest |

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ dags/                        # Airflow DAGs for pipeline orchestration
â”œâ”€â”€ configs/                     # Configuration files for Spark, Hive, and Hadoop
â”œâ”€â”€ spark-scripts/               # Scripts for data ingestion, transformation, and ML models
â”‚   â”œâ”€â”€ load.py                  # Data loading script
â”‚   â”œâ”€â”€ transform.py             # Data transformation script
â”‚   â”œâ”€â”€ ml_customer_segmentation.py      # K-Means clustering
â”‚   â”œâ”€â”€ ml_sales_forecasting.py          # Sales prediction
â”‚   â”œâ”€â”€ ml_product_recommendations.py    # ALS recommendations
â”‚   â”œâ”€â”€ ml_churn_prediction.py           # Churn risk scoring
â”‚   â””â”€â”€ ml_anomaly_detection.py          # Anomaly detection
â”œâ”€â”€ data/                        # Folder to store raw data
â”œâ”€â”€ output/                      # Folder for results
â”œâ”€â”€ streamlit/                   # Streamlit app for data visualization with ML insights
â”œâ”€â”€ docker-compose.yml           # Docker Compose file to orchestrate services
â””â”€â”€ ML_IMPLEMENTATION.md         # Comprehensive ML documentation

```

---

## ğŸš€ Quick Start Guide

### Prerequisites

- Docker
- Docker Compose
- Minimum 16GB RAM recommended
- Git

### Installation Steps

1. **Clone the Repository**
   ```bash
   git clone https://github.com/AbderrahmaneOd/spark-hive-airflow-ecommerce-pipeline.git
   cd spark-hive-airflow-ecommerce-pipeline
   ```

2. **Launch Infrastructure**
   ```bash
   docker-compose up -d
   ```

3. **Access Services**

   | Service | URL | Credentials |
   |---------|-----|-------------|
   | HDFS Namenode | http://localhost:9870 | - |
   | YARN ResourceManager | http://localhost:8088 | - |
   | Spark Master | http://localhost:8080 | - |
   | Spark Worker | http://localhost:8081 | - |
   | Zeppelin | http://localhost:8082 | - |
   | Airflow | http://localhost:3000 | admin@gmail.com / admin |

4. **Configure Spark Connection**
   - Navigate to Airflow UI
   - Go to Admin > Connections
   - Edit "spark_default"
     * Host: spark://namenode
     * Port: 7077

---

## ğŸ” Pipeline Workflow

The Airflow DAG demonstrates a complete data and ML workflow:
1. **Wait for Data File** - FileSensor monitors for new data
2. **Upload to HDFS** - BashOperator copies data to distributed storage
3. **Load Data** - Spark reads CSV and creates raw Hive table
4. **Transform Data** - Spark performs ETL and creates analytical tables
5. **ML Models (Run in Parallel)** - 5 ML models generate insights:
   - Customer Segmentation (K-Means)
   - Sales Forecasting (Linear Regression)
   - Product Recommendations (ALS)
   - Churn Prediction (Random Forest)
   - Anomaly Detection (Z-Score)

---

## ğŸ¤– Machine Learning Models

### 1. Customer Segmentation ğŸ‘¥
- **Algorithm**: K-Means Clustering (k=4)
- **Features**: Total Purchases, Number of Transactions
- **Output**: 4 customer segments (High/Medium/Low Value, New Buyers)
- **Business Use**: Targeted marketing, personalized campaigns

### 2. Sales Forecasting ğŸ“ˆ
- **Algorithm**: Linear Regression
- **Features**: Month Index, Lag features (1-3 months)
- **Output**: Next 3 months sales predictions
- **Business Use**: Inventory planning, revenue projections

### 3. Product Recommendations ğŸ¯
- **Algorithm**: ALS Collaborative Filtering
- **Features**: Customer-Product-Rating matrix
- **Output**: Top 10 product recommendations per customer
- **Business Use**: Cross-selling, personalization, email marketing

### 4. Churn Prediction âš ï¸
- **Algorithm**: Random Forest Classifier
- **Features**: RFM (Recency, Frequency, Monetary)
- **Output**: Churn probability and risk level (High/Medium/Low)
- **Business Use**: Retention campaigns, customer intervention

### 5. Anomaly Detection ğŸš¨
- **Algorithm**: Statistical Z-Score Analysis
- **Features**: Quantity, Unit Price, Total Amount
- **Output**: Anomalous transactions with severity levels
- **Business Use**: Fraud detection, data quality monitoring

**ğŸ“š For detailed ML documentation, see [ML_IMPLEMENTATION.md](ML_IMPLEMENTATION.md)**

---
   
## ğŸ“Š Data Visualization
The project includes an enhanced Streamlit app with both business analytics and ML insights:

**Business Metrics**:
- Total Transactions, Quantity Sold, Revenue
- Sales by Country and Year
- Top Selling Products
- Sales Trend Over Time

**ML Insights**:
- Customer Segment Distribution (pie charts)
- Sales Forecast vs. Actual (line charts)
- Top Product Recommendations (tables)
- Churn Risk Analysis (bar charts, gauges)
- Transaction Anomalies (severity charts, alerts)

**Interactive Features**:
- Country-based filtering
- Date range selection
- Downloadable filtered data
- Real-time ML insights

---
 
## ğŸ–¼ï¸ Project Outputs

### Airflow Pipeline Visualization
![Airflow Pipeline Graph](/output/ecommerce_data_pipeline-Airflow.png)
*Airflow DAG (Directed Acyclic Graph) showing the pipeline workflow*

### Airflow Task Dependencies
![Airflow Task Dependencies](/output/ecommerce_data_pipeline-Graph-Airflow.png)
*Detailed view of task dependencies in the data pipeline*

### Streamlit E-Commerce Dashboard
![Streamlit Dashboard](/output/E-Commerce-Dashboard-Streamlit.png)
*Interactive dashboard for e-commerce sales analytics*
